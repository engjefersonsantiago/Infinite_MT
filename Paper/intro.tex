\section{Introduction}\label{sec:intro}

The Software-Defined Networking (SDN) paradigm has brought programmability to the once rigid network ecosystem.
By allowing both control and data planes to evolve independently, SDN has opened new research avenues in networking, including data plane programming.
Notably, the P4 language is a result of the SDN convergence~\cite{Bosshart:14}.
P4 leverages OpenFlow by allowing network admins to program their network in a custom fashion.
Thanks to P4 and recent programmable dataplanes~\cite{Bosshart:13}, network admins can now deploy custom protocols by simply reprogramming the network switches according to their evolving needs, without the need of deploying expensive new hardware. 

However, current requirements of data center networks are such that even state-of-the-art programmable ASICs switches~\cite{tofino:18} cannot solely meet them.
For instance, next generation mobile communication (5G) will require multi-million active sessions ($>$\SI{5}{\mega\nothing}) at terabit rates, stringently low end-to-end latency ($<$\SI{1}{\milli\second}), and likely, new P4-defined custom protocols.

Recently, some research has suggested using heterogeneous programmable data planes (HDPs) to alleviate data center network switch bottlenecks~\cite{p4eu:18}.
Indeed, using complementary and distinct packet forwarding devices increases the overall switch processing capabilities.
However, research regarding HDPs is still in its infancy with many open questions, such as heterogeneous compilers, the issue of mismatched processing capabilities among devices, and distributed match-tables management.

In this work, we address the issue of distributed match-action table management in HDPs comprising a high speed programmable ASIC, an FPGA, and a host CPU, as illustrated in Figure~\ref{fig:high_level_network}.
To that end, we borrow the cache hierarchy concept of regular computer systems.
In our system, a first-level cache is the high-performance but memory limited programmable ASIC.
The FPGA plays the role of second-level cache, and finally the CPU is the last-level cache.
However, match-action caching is fairly different from CPU caching.
First, temporal and spatial data locality are less predictable in network systems.
Second, memory is scarce and the processing flexibility is limited in network switches, thus, complex cache policies may not be suitable.
Third, due to dynamic traffic changes, a cache system needs to rapidly adapt to diversified loads.
As a consequence, traditional computer caching schemes cannot be applied for a heterogeneous match table cache as is.

\begin{figure}[]
	\centering
	\input{arch_scheme.tex}
	\caption{Reference cache system}
	\label{fig:high_level_network}
\end{figure}

%Thus, we propose a match-action cache policer split into the forwarding devices.
%We use online traffic hitters to estimate which match-action entries are candidates to be promoted/evicted to/from another cache level.
%We are inspired in previous works on flow caching \cite{casado:2008,Katta:2014,Pfaff:15} and on heuristic dataplane-based traffic hitters \cite{Sivaraman:17} aiming to maintain line-rate throughput and required cache update rate while minimizing the usage of scarce memory resources available in each device and reducing processing latency.

Thus, we conduct a series of experiments to evaluate the feasibility of such an HDP caching system.
We start by re-characterizing current data center network traffic in order to understanding network traffic temporal locality.
Following the traffic analysis, we have conducted cache simulations to choose realistic cache policies to be implemented in HDPs.
Finally, we have evaluated which cache policies are implementable in current programmable dataplanes and how can they be described in P4.

To the best of our knowledge, our work is the first to consider a match-action  table management for HDPs.
The contributions of this work are as follows: 

\begin{figure*}[]
	\centering
	\subfloat[Heavy hitter analysis]{
		\includegraphics[width=0.33\textwidth]{fig_cdf.pdf}
		\label{fig:cdf}
	}
	\subfloat[Flow duration]{
		\includegraphics[width=0.33\textwidth]{fig_duration.pdf}
		\label{fig:flow_duration}
	}
	\subfloat[Average flow size]{
		\includegraphics[width=0.33\textwidth]{fig_avg_flow_size.pdf}
		\label{fig:flow_size}
	}
	\caption{CAIDA trace summary}
	\label{fig:traces}
\end{figure*}

\begin{itemize}[noitemsep,topsep=0pt]
	\item an open-source match-action cache policer for HDPs;
	\item a real-world network traffic analisys to derive match-action cache premises~(\S\ref{sec:traffic});
	\item an evaluation of cache policies in the context of programmable dataplanes~(\S\ref{sec:policies}); and
	\item a model to estimate the performance and overhead of a match-action cache system in an HDP~(\S\ref{sec:cache_results}).
\end{itemize}

